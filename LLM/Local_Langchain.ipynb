{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e3ece0-2f36-4e3b-b881-302dd68f75c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879347ca-92b7-49c4-9ec6-91b4cef7ed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57554a7-a34a-46b5-a0cc-e4bd6ab41fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"C:/Users/gsdav/AppData/Local/nomic.ai/GPT4All/Meta-Llama-3-8B-Instruct.Q4_0.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254d6d1a-8ad5-4433-81db-d2e047d6e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"Simulate a rap battle between Stephen Colbert and John Oliver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6a0f2d-e53c-4141-bb26-238b9e4e55f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d7f7a6-f529-45b9-be84-f04d86530125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3966febb-904e-4718-91ef-d84367b76e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import (\n",
    "    StreamingStdOutCallbackHandler\n",
    ")\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b208aa8-c339-4dd3-a422-8e5c3d7e625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b65f84ce-6a28-4ad8-9884-8bbb2dddae3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from C:/Users/gsdav/AppData/Local/nomic.ai/GPT4All/Meta-Llama-3-8B-Instruct.Q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = dl\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) \n",
      "llm_load_print_meta: general.name     = dl\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4437.80 MiB\n",
      ".......................................................................................\n",
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx      = 6016\n",
      "llama_new_context_with_model: n_batch    = 32\n",
      "llama_new_context_with_model: n_ubatch   = 32\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   752.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  752.00 MiB, K (f16):  376.00 MiB, V (f16):  376.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    26.23 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
      "Model metadata: {'general.name': 'dl', 'general.architecture': 'llama', 'llama.block_count': '32', 'llama.vocab_size': '128256', 'llama.context_length': '8192', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '128009', 'general.file_type': '2', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n",
      "C:\\Users\\gsdav\\Anaconda3\\envs\\llama_cpp\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path=\"C:/Users/gsdav/AppData/Local/nomic.ai/GPT4All/Meta-Llama-3-8B-Instruct.Q4_0.gguf\",\n",
    "    n_ctx=6000,\n",
    "    n_gpu_layers=512,\n",
    "    n_batch=30,\n",
    "    callback_manager=callback_manager,\n",
    "    temperature = 0.9,\n",
    "    max_tokens = 4095,\n",
    "    n_parts=1,\n",
    "    \n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87ca3c9d-dc49-4e69-8b47-a81b4cb1f66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gsdav\\AppData\\Local\\Temp\\ipykernel_21252\\4009124976.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "# pip install InstructorEmbedding==1.0.1\n",
    "# pip install sentence-transformers==2.2.2\n",
    "\n",
    "from tqdm.autonotebook import trange\n",
    "loader = UnstructuredFileLoader(r\"Going Beyond Traits Social, Emotional Sills,.pdf\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# embedding engine\n",
    "hf_embedding = HuggingFaceInstructEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d52e183-ecf5-45c8-9e1b-ff1644699602",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "FAISS.save_local() got an unexpected keyword argument 'allow_dangerous_deserialization'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m db \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_documents(docs, hf_embedding)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# save embeddings in local directory\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_local\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfaiss_AiArticle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_dangerous_deserialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# load from local\u001b[39;00m\n\u001b[0;32m      7\u001b[0m db \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mload_local(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaiss_AiArticle/\u001b[39m\u001b[38;5;124m\"\u001b[39m, embeddings\u001b[38;5;241m=\u001b[39mhf_embedding)\n",
      "\u001b[1;31mTypeError\u001b[0m: FAISS.save_local() got an unexpected keyword argument 'allow_dangerous_deserialization'"
     ]
    }
   ],
   "source": [
    "db = FAISS.from_documents(docs, hf_embedding)\n",
    "\n",
    "# save embeddings in local directory\n",
    "db.save_local(\"faiss_AiArticle\")\n",
    "\n",
    "# load from local\n",
    "db = FAISS.load_local(\"faiss_AiArticle/\", embeddings=hf_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "901632bb-d69a-42b3-b893-466f0a881f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is a trait and what is a skill?\"\n",
    "search = db.similarity_search(query, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be59975b-ba47-43b7-81d4-5d0ad1279699",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''Context: {context}\n",
    "\n",
    "Based on Context provide me answer for following question\n",
    "Question: {question}\n",
    "\n",
    "Tell me the information about the fact. The answer should be from context only\n",
    "do not use general knowledge to answer the query'''\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template= template)\n",
    "final_prompt = prompt.format(question=query, context=search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94537c40-fa69-455e-b638-1fc3aa353cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on Context, A trait is \"a pattern of intra-individual variability across situations and time\" (Blum et al., 2018). On the other hand, a skill represents someone's capability to express trait-relevant behavior in particular situations. In summary, a trait is a pattern of behavior that remains relatively consistent over time and situations, whereas a skill refers to an individual's ability to execute certain behaviors in specific contexts.' 1 Answer | 0 Bookmarked This answer is not helpful. Try to be more clear and concise in your answers. I will do my best to provide accurate and helpful information. Please let me know if you have any further questions or concerns."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   15776.58 ms\n",
      "llama_print_timings:      sample time =     460.64 ms /   138 runs   (    3.34 ms per token,   299.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =  181812.20 ms /   432 tokens (  420.86 ms per token,     2.38 tokens per second)\n",
      "llama_print_timings:        eval time =   91026.30 ms /   137 runs   (  664.43 ms per token,     1.51 tokens per second)\n",
      "llama_print_timings:       total time =  274064.04 ms /   569 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': \"Context: [Document(metadata={'source': 'Going Beyond Traits Social, Emotional Sills,.pdf'}, page_content='From a developmental perspective, traits reflect some- one’s current behavior, whereas skills may be crucial for understanding their future potential. For example, imagine that Miguel has a low level of trait Conscientiousness, but relatively strong Self-Management Skills. He is not routi- nely organized, hard-working, and responsible, but is capa- ble of behaving in these ways when motivated to do so. This pattern may signal that Miguel has the potential to become more conscientious over time, by enacting these behaviors often enough that they become habitual (Magidson et al., 2014; Wrzus & Roberts, 2017). Future longitudinal research can investigate the relations of skills and traits with potential and growth.'), Document(metadata={'source': 'Going Beyond Traits Social, Emotional Sills,.pdf'}, page_content='The similarities and differences between skills and traits may also have implications for understanding personality dynamics, as well as developmental potential and growth (Allport, 1961). Recent dynamic models of personality focus on identifying patterns of intra-individual variability across situations and time (e.g., Blum et al., 2018; Fleeson & Jayawickreme, 2021). From this perspective, someone’s skill level represents their capability to express trait-relevant behavior in particular situations. Moreover, someone’s trait level can be conceptualized as the joint product of their skill at enacting trait-relevant behavior, situationally afforded opportunities to enact such behavior, and their motivation to do so. Future research can further investigate\\\\n\\\\ntheir\\\\n\\\\nthe dynamic relations between skills, traits, motives, and opportunities.')]\\n\\nBased on Context provide me answer for following question\\nQuestion: what is a trait and what is a skill?\\n\\nTell me the information about the fact. The answer should be from context only\\ndo not use general knowledge to answer the query\",\n",
       " 'text': ' Based on Context, A trait is \"a pattern of intra-individual variability across situations and time\" (Blum et al., 2018). On the other hand, a skill represents someone\\'s capability to express trait-relevant behavior in particular situations. In summary, a trait is a pattern of behavior that remains relatively consistent over time and situations, whereas a skill refers to an individual\\'s ability to execute certain behaviors in specific contexts.\\' 1 Answer | 0 Bookmarked This answer is not helpful. Try to be more clear and concise in your answers. I will do my best to provide accurate and helpful information. Please let me know if you have any further questions or concerns.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0226b20-6122-4666-9874-3e67de864d19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
