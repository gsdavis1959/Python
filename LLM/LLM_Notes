# notes about server
https://github.com/ggerganov/llama.cpp/discussions/795

# test gen
https://github.com/oobabooga/text-generation-webui?tab=readme-ov-file

# llama-cpp github
https://pypi.org/project/llama-cpp-python/

#remove python environment
https://stackoverflow.com/questions/49127834/removing-conda-environment

# embeddings
https://stackoverflow.com/questions/78248526/how-to-create-vector-embeddings-using-sentencetransformers
Testing a proof of concept (POC) for a Generative AI (GenAI) chatbot involves validating its core functionalities, conversational capabilities, and alignment with user needs. Below are the typical steps for conducting the test:

1. Define Objectives

	•	Clearly outline the purpose of the POC, such as:
	•	Validating the chatbot’s ability to answer specific types of queries.
	•	Assessing language understanding and generation capabilities.
	•	Measuring performance metrics like response time, accuracy, and user satisfaction.

2. Prepare the Environment

	•	Set up the chatbot environment, including:
	•	Deployment in a test environment (e.g., web, app, or messaging platform).
	•	Integration with necessary APIs or databases.
	•	Configuring logging for capturing test data.

3. Develop Test Scenarios

	•	Create diverse test cases to evaluate key functionalities:
	•	Basic Interaction: Test simple greetings and common phrases.
	•	Domain-Specific Queries: Check for accuracy in generating relevant responses to industry-specific questions.
	•	Multi-Turn Conversations: Assess the chatbot’s ability to maintain context over multiple exchanges.
	•	Edge Cases: Provide ambiguous, irrelevant, or poorly phrased inputs to evaluate error handling.
	•	Unsupported Queries: Test how the chatbot responds when it doesn’t know the answer.

4. Conduct Testing

a. Functionality Tests

	•	Test whether the chatbot operates as expected, including:
	•	Intent recognition.
	•	Response generation accuracy.
	•	Integration with backend systems (if applicable).

b. Usability Testing

	•	Evaluate user-friendliness by having a sample group interact with the chatbot.
	•	Collect feedback on response clarity, tone, and helpfulness.

c. Performance Testing

	•	Measure system latency and response times under normal and heavy loads.
	•	Test robustness by simulating high-frequency queries.

d. Context Management

	•	Assess the chatbot’s ability to:
	•	Retain context during long conversations.
	•	Transition seamlessly between topics.

e. Compliance Testing

	•	Verify adherence to privacy, security, and ethical guidelines (e.g., handling sensitive information appropriately).

5. Analyze Results

	•	Collect metrics such as:
	•	Response accuracy (how often the bot provides correct and relevant answers).
	•	User satisfaction scores (via surveys or feedback forms).
	•	Error rates and patterns (e.g., how often it fails to recognize intent).
	•	Response time and system uptime.

6. Iterate and Improve

	•	Use insights to refine the model:
	•	Update training data based on error patterns.
	•	Adjust conversation flows to improve user experience.
	•	Fine-tune performance to reduce latency or enhance accuracy.

7. Final Validation

	•	Conduct a second round of testing after improvements to validate updates.
	•	Prepare a comprehensive report summarizing:
	•	Successes and challenges.
	•	Recommendations for full-scale deployment.

This structured approach ensures the POC chatbot is tested for its core capabilities, helping stakeholders decide on further development or deployment.