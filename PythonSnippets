# count unique with multiple columns
df.groupby(['col_a', 'col_b']).ngroups

# transformer model
https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1/tree/main

from google.colab import files
files.upload()

# remove punctuation and digits in a pandas column 
df.poem = df.poem.apply(lambda x: x.translate(None, string.punctuation))
df.poem = df.poem.apply(lambda x: x.translate(None, string.digits))
# using re
df['E'] = df['B'].map(lambda x: re.sub(r'\W+', '', x))

# read muliple files
import os
os.listdir()
os.chdir(‘directory where the files are located’)
files = [file for file in os.listdir()]
df = pd.concat(map(pd.read_csv, files), ignore_index=True)
df.head()

# delete a column
del df[‘Age’]

# sorting a dataframe
df.sort(column=’Age’)

# and filter
df[(df["class"] == 'A') & (df["score"] > 80)]

# filter based on part of a string
# if you need to avoid NaNs df2["Interests"] = df2["Interests"].fillna("")
df[df["Interests"].str.contains("Reading")]
df[df["Interests"].str.contains("reading", case=False)] # ignore case
df[df["Interests"].str.contains(".*V.*")] # with wildcard


# save jupyter notebook to html

%%javascript
IPython.notebook.save_notebook()

import os
os.system('jupyter nbconvert --to html yourNotebook.ipynb')
# or with output
os.system('jupyter nbconvert --execute --to html notebook.ipynb')

# open oft file
import win32com.client as win32
outlook = win32.Dispatch('outlook.application')
mail = outlook.CreateItemFromTemplate(0)

#mail.Open() does not exist... but what instead?
mail.Open('Template1.oft')

mail.HtmlBody = alterEmail(mail.HtmlBody)
mail.Display(True)
mail.Save()

# case when examples
conditions = [
    (df["age"].lt(10)),
    (df["age"].ge(10) & df["age"].lt(20)),
    (df["age"].ge(20) & df["age"].lt(30)),
    (df["age"].ge(30) & df["age"].lt(50)),
    (df["age"].ge(50)),
]
choices = ["baby", "kid", "young", "mature", "grandpa"]

df["elderly"] = np.select(conditions, choices)

# Results in:
#      name  age  preTestScore  postTestScore  elderly
#  0  Jason   42             4             25   mature
#  1  Molly   52            24             94  grandpa
#  2   Tina   36            31             57   mature
#  3   Jake   24             2             62    young
#  4    Amy   73             3             70  grandpa

# using pyjanitor
# pip install git+https://github.com/pyjanitor-devs/pyjanitor.git
import pandas as pd
import janitor as jn

df.case_when(
df.age.lt(10), 'baby', # 1st condition, result
df.age.between(10, 20, 'left'), 'kid', # 2nd condition, result
df.age.between(20, 30, 'left'), 'young', # 3rd condition, result
 df.age.between(30, 50, 'left'), 'mature', # 4th condition, result
'grandpa',  # default if none of the conditions match
 column_name = 'elderly') # column name to assign to
 
    name  age  preTestScore  postTestScore  elderly
0  Jason   42             4             25   mature
1  Molly   52            24             94  grandpa
2   Tina   36            31             57   mature
3   Jake   24             2             62    young
4    Amy   73             3             70  grandpa


# pipe

# Changing the 'host_is_superhost' column values.
def boolean(df):
    df['host_is_superhost'] = df['host_is_superhost'].replace({'f': False, 't':True})
    return df

# Correcting the numbers appearance in 'price' column.
def remove_dollar(df):
    df['price'] = (df['price'].replace({'\$': '', ',':''}, regex = True)
                   .replace({'.':''},).astype('float'))
    return df

# Setting 'Downtown' as the actual neighborhood for residences with 
# touristic place names in 'neighbourhood' column.
def downtown(df):
    touristic = ['Downtown Crossing', 'Government Center', 'Leather District', 
             'Chinatown','Theater District','Financial District']
    
    substitute = {location : 'Downtown' for location in touristic}
    df['neighbourhood'] = df['neighbourhood'].replace(substitute)
    return df
    
# Applying each function generated with 'pipe'.
(airbnb.pipe(boolean)
        .pipe(remove_dollar)
        .pipe(downtown)).head()


# scatter matrix
# You can use a color map in order to customize the dots appearance.
from matplotlib import cm
cmap = cm.get_cmap('gnuplot')
pd.plotting.scatter_matrix(X_train, c = y_train, marker = 'o' ,cmap = cmap, figsize = (8, 8), diagonal = 'kde');

# connect to SharePoint list or file
from office365.runtime.auth.user_credential import UserCredential
from office365.sharepoint.client_context import ClientContext

site_url = "https://yoursharepointsite.com" + f"/sites/{documentsite}/"
ctx = ClientContext(site_url).with_credentials(UserCredential(username, password=password))
ctx.load(ctx.web)
ctx.execute_query()

from office365.sharepoint.files import file
import io

file_url = file_dl if dynamic else f'/sites/{documentsite}/{file_dl}'
response = file.File.open_binary(ctx, file_url)
bytes_file_obj = io.BytesIO()
bytes_file_obj.write(response.content)
bytes_file_obj.seek(0)
return bytes_file_obj

# sample groups
df.groupby('Group_Id').sample(n=1)

# compare dfs
df = pd.concat([df1, df2]).groupby('UserName', as_index=False).sum()

# change from one date to another

#Create datetime column
df['DateTime'] = pd.to_datetime(df['Date'], format='%b-%y')

#Set it as index
df.set_index('DateTime', inplace=True)

#Then shift by month frequency:
df['otm'] = df['Value'] - df['Value'].shift(1, freq='MS')
df['oty'] = df['Value'] - df['Value'].shift(12, freq='MS')

#Set it as index
df.set_index('DateTime', inplace=True)

#Then shift by month frequency:
df['otm'] = df['Value'] - df['Value'].shift(1, freq='MS')
df['oty'] = df['Value'] - df['Value'].shift(12, freq='MS')

df['otm'] = df.Value.diff()
df['oty'] = df.Value.diff(12)

# compare two dataframes
col_groups = [c.columns for _, c in df.groupby(df.columns.str[0], axis=1)]
df['diff'] = pd.Series(np.sum([(df[l] != df[r]).map({True: f'{l}-{r}',False:''}) + ',' for l, r in col_groups], axis=0)).str.strip(',')

# compare rows
import numpy as np
import pandas as pd
data = [{'a1': 1, 'b1': 2, 'a2':1, 'b2':2},
        {'a1':1, 'b1': 2, 'a2': 1, 'b2':3},
        {'a1':1, 'b1': 2, 'a2':3 , 'b2':4}]
df = pd.DataFrame(data)
compare = [('a1','a2'),('b1','b2')]

def compare_rows(row):
    differences = ['-'.join(comp)
                   for comp in compare
                   if row[comp[0]] != row[comp[1]]]
    return ','.join(differences)

df['diff'] = df.apply(compare_rows, axis=1)

# conversion of R dpylr mutate 
import pandas as pd

data = pd.read_csv('data.csv')

data = data.groupby('x').apply(lambda group: group.assign(value=group['value']*2))

# groupby to remove a multi index
data.groupby('month', as_index=False).agg({"duration": "sum"})

# using transform
df["Order_Total"] = df.groupby('order')["ext price"].transform('sum')
df["Percent_of_Order"] = df["ext price"] / df["Order_Total"]
# in one line
df["Percent_of_Order"] = df["ext price"] / df.groupby('order')["ext price"].transform('sum')


# topic model code
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.datasets import fetch_20newsgroups
from sklearn.decomposition import NMF, LatentDirichletAllocation

def display_topics(model, feature_names, no_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print "Topic %d:" % (topic_idx)
        print " ".join([feature_names[i]
                        for i in topic.argsort()[:-no_top_words - 1:-1]])

dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))
documents = dataset.data

no_features = 1000

# NMF is able to use tf-idf
tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')
tfidf = tfidf_vectorizer.fit_transform(documents)
tfidf_feature_names = tfidf_vectorizer.get_feature_names()

# LDA can only use raw term counts for LDA because it is a probabilistic graphical model
tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')
tf = tf_vectorizer.fit_transform(documents)
tf_feature_names = tf_vectorizer.get_feature_names()

no_topics = 20

# Run NMF
nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)

# Run LDA
lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)

no_top_words = 10
display_topics(nmf, tfidf_feature_names, no_top_words)
display_topics(lda, tf_feature_names, no_top_words)

# create activity class to module function
def reindex_tasks(task_list):
    new_task_list = []
    for task in task_list:
        if task['type'] == 'task':
            current_task_id = task['id']
            new_task_list.append(task)
        else:
            subtask = task.copy()
            subtask['id'] = current_task_id
            new_task_list.append(subtask)
    return new_task_list

import pandas as pd

# assuming your DataFrame is named df and the column with the task list is named 'tasks'
df['new_tasks'] = df['tasks'].apply(reindex_tasks)


# connect to ms access
https://datatofish.com/how-to-connect-python-to-ms-access-database-using-pyodbc/
https://stackoverflow.com/questions/12704305/return-column-names-from-pyodbc-execute-statement
https://stackoverflow.com/questions/16519385/output-pyodbc-cursor-results-as-python-dictionary
# concatenate rows with group by
https://stackoverflow.com/questions/27298178/concatenate-strings-from-several-rows-using-pandas-groupby
# update values from data in another dataframe
https://stackoverflow.com/questions/49928463/python-pandas-update-a-dataframe-value-from-another-dataframe
# factor lists
https://www.tutorialspoint.com/python-make-pair-from-two-list-such-that-elements-are-not-same-in-pairs
