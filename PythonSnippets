# count unique with multiple columns
df.groupby(['col_a', 'col_b']).ngroups

# transformer model
https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1/tree/main

from google.colab import files
files.upload()

# remove punctuation and digits in a pandas column 
df.poem = df.poem.apply(lambda x: x.translate(None, string.punctuation))
df.poem = df.poem.apply(lambda x: x.translate(None, string.digits))
# using re
df['E'] = df['B'].map(lambda x: re.sub(r'\W+', '', x))

# read muliple files
import os
os.listdir()
os.chdir(‘directory where the files are located’)
files = [file for file in os.listdir()]
df = pd.concat(map(pd.read_csv, files), ignore_index=True)
df.head()

# delete a column
del df[‘Age’]

# sorting a dataframe
df.sort(column=’Age’)

# and filter
df[(df["class"] == 'A') & (df["score"] > 80)]

# filter based on part of a string
# if you need to avoid NaNs df2["Interests"] = df2["Interests"].fillna("")
df[df["Interests"].str.contains("Reading")]
df[df["Interests"].str.contains("reading", case=False)] # ignore case
df[df["Interests"].str.contains(".*V.*")] # with wildcard


# save jupyter notebook to html

%%javascript
IPython.notebook.save_notebook()

import os
os.system('jupyter nbconvert --to html yourNotebook.ipynb')
# or with output
os.system('jupyter nbconvert --execute --to html notebook.ipynb')

# open oft file
import win32com.client as win32
outlook = win32.Dispatch('outlook.application')
mail = outlook.CreateItemFromTemplate(0)

#mail.Open() does not exist... but what instead?
mail.Open('Template1.oft')

mail.HtmlBody = alterEmail(mail.HtmlBody)
mail.Display(True)
mail.Save()

# case when examples
conditions = [
    (df["age"].lt(10)),
    (df["age"].ge(10) & df["age"].lt(20)),
    (df["age"].ge(20) & df["age"].lt(30)),
    (df["age"].ge(30) & df["age"].lt(50)),
    (df["age"].ge(50)),
]
choices = ["baby", "kid", "young", "mature", "grandpa"]

df["elderly"] = np.select(conditions, choices)

# Results in:
#      name  age  preTestScore  postTestScore  elderly
#  0  Jason   42             4             25   mature
#  1  Molly   52            24             94  grandpa
#  2   Tina   36            31             57   mature
#  3   Jake   24             2             62    young
#  4    Amy   73             3             70  grandpa

# using pyjanitor
# pip install git+https://github.com/pyjanitor-devs/pyjanitor.git
import pandas as pd
import janitor as jn

df.case_when(
df.age.lt(10), 'baby', # 1st condition, result
df.age.between(10, 20, 'left'), 'kid', # 2nd condition, result
df.age.between(20, 30, 'left'), 'young', # 3rd condition, result
 df.age.between(30, 50, 'left'), 'mature', # 4th condition, result
'grandpa',  # default if none of the conditions match
 column_name = 'elderly') # column name to assign to
 
    name  age  preTestScore  postTestScore  elderly
0  Jason   42             4             25   mature
1  Molly   52            24             94  grandpa
2   Tina   36            31             57   mature
3   Jake   24             2             62    young
4    Amy   73             3             70  grandpa


# pipe

# Changing the 'host_is_superhost' column values.
def boolean(df):
    df['host_is_superhost'] = df['host_is_superhost'].replace({'f': False, 't':True})
    return df

# Correcting the numbers appearance in 'price' column.
def remove_dollar(df):
    df['price'] = (df['price'].replace({'\$': '', ',':''}, regex = True)
                   .replace({'.':''},).astype('float'))
    return df

# Setting 'Downtown' as the actual neighborhood for residences with 
# touristic place names in 'neighbourhood' column.
def downtown(df):
    touristic = ['Downtown Crossing', 'Government Center', 'Leather District', 
             'Chinatown','Theater District','Financial District']
    
    substitute = {location : 'Downtown' for location in touristic}
    df['neighbourhood'] = df['neighbourhood'].replace(substitute)
    return df
    
# Applying each function generated with 'pipe'.
(airbnb.pipe(boolean)
        .pipe(remove_dollar)
        .pipe(downtown)).head()


# scatter matrix
# You can use a color map in order to customize the dots appearance.
from matplotlib import cm
cmap = cm.get_cmap('gnuplot')
pd.plotting.scatter_matrix(X_train, c = y_train, marker = 'o' ,cmap = cmap, figsize = (8, 8), diagonal = 'kde');

# connect to SharePoint list or file
from office365.runtime.auth.user_credential import UserCredential
from office365.sharepoint.client_context import ClientContext

site_url = "https://yoursharepointsite.com" + f"/sites/{documentsite}/"
ctx = ClientContext(site_url).with_credentials(UserCredential(username, password=password))
ctx.load(ctx.web)
ctx.execute_query()

from office365.sharepoint.files import file
import io

file_url = file_dl if dynamic else f'/sites/{documentsite}/{file_dl}'
response = file.File.open_binary(ctx, file_url)
bytes_file_obj = io.BytesIO()
bytes_file_obj.write(response.content)
bytes_file_obj.seek(0)
return bytes_file_obj

# sample groups
df.groupby('Group_Id').sample(n=1)

# compare dfs
df = pd.concat([df1, df2]).groupby('UserName', as_index=False).sum()

# change from one date to another

#Create datetime column
df['DateTime'] = pd.to_datetime(df['Date'], format='%b-%y')

#Set it as index
df.set_index('DateTime', inplace=True)

#Then shift by month frequency:
df['otm'] = df['Value'] - df['Value'].shift(1, freq='MS')
df['oty'] = df['Value'] - df['Value'].shift(12, freq='MS')

#Set it as index
df.set_index('DateTime', inplace=True)

#Then shift by month frequency:
df['otm'] = df['Value'] - df['Value'].shift(1, freq='MS')
df['oty'] = df['Value'] - df['Value'].shift(12, freq='MS')

df['otm'] = df.Value.diff()
df['oty'] = df.Value.diff(12)

# compare two dataframes
col_groups = [c.columns for _, c in df.groupby(df.columns.str[0], axis=1)]
df['diff'] = pd.Series(np.sum([(df[l] != df[r]).map({True: f'{l}-{r}',False:''}) + ',' for l, r in col_groups], axis=0)).str.strip(',')

# compare rows
import numpy as np
import pandas as pd
data = [{'a1': 1, 'b1': 2, 'a2':1, 'b2':2},
        {'a1':1, 'b1': 2, 'a2': 1, 'b2':3},
        {'a1':1, 'b1': 2, 'a2':3 , 'b2':4}]
df = pd.DataFrame(data)
compare = [('a1','a2'),('b1','b2')]

def compare_rows(row):
    differences = ['-'.join(comp)
                   for comp in compare
                   if row[comp[0]] != row[comp[1]]]
    return ','.join(differences)

df['diff'] = df.apply(compare_rows, axis=1)

# conversion of R dpylr mutate 
import pandas as pd

data = pd.read_csv('data.csv')

data = data.groupby('x').apply(lambda group: group.assign(value=group['value']*2))

# groupby to remove a multi index
data.groupby('month', as_index=False).agg({"duration": "sum"})

# using transform
df["Order_Total"] = df.groupby('order')["ext price"].transform('sum')
df["Percent_of_Order"] = df["ext price"] / df["Order_Total"]
# in one line
df["Percent_of_Order"] = df["ext price"] / df.groupby('order')["ext price"].transform('sum')


# topic model code
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.datasets import fetch_20newsgroups
from sklearn.decomposition import NMF, LatentDirichletAllocation

def display_topics(model, feature_names, no_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print "Topic %d:" % (topic_idx)
        print " ".join([feature_names[i]
                        for i in topic.argsort()[:-no_top_words - 1:-1]])

dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))
documents = dataset.data

no_features = 1000

# NMF is able to use tf-idf
tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')
tfidf = tfidf_vectorizer.fit_transform(documents)
tfidf_feature_names = tfidf_vectorizer.get_feature_names()

# LDA can only use raw term counts for LDA because it is a probabilistic graphical model
tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')
tf = tf_vectorizer.fit_transform(documents)
tf_feature_names = tf_vectorizer.get_feature_names()

no_topics = 20

# Run NMF
nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)

# Run LDA
lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)

no_top_words = 10
display_topics(nmf, tfidf_feature_names, no_top_words)
display_topics(lda, tf_feature_names, no_top_words)

# create activity class to module function

import pandas as pd

# Sample data frame
df = pd.DataFrame({
    'Activity ID': [1, 2, 3, 4, 5, 6, 7, 8, 9],
    'Activity Type': ['Class', 'Study', 'Study', 'Class', 'Study', 'Class', 'Study', 'Study', 'Class']
})

# Create a new column named 'New Activity ID'
df['New Activity ID'] = ''

# Initialize a variable to store the current 'Class' activity ID
current_class_id = None

# Iterate through the rows of the data frame
for index, row in df.iterrows():
    activity_id = row['Activity ID']
    activity_type = row['Activity Type']
    
    # Check if the activity type is 'Class'
    if activity_type == 'Class':
        current_class_id = activity_id
    
    # Assign the current 'Class' activity ID to the 'New Activity ID' column
    df.at[index, 'New Activity ID'] = current_class_id

# Print the updated data frame
print(df)

# f strings
pi = 3.14159
e = 2.71828

print(f'{pi:.2f} {e:.4f}')
# 3.14 2.7183
# %
print(f'{0.25:.0%}')    # 25%

# commas
print(f'{1000:,}')      # 1,000
print(f'{10000:,}')     # 10,000
print(f'{100000:,}')    # 100,000
print(f'{1000000:,}')   # 1,000,000

print(f'{4:+}')      # +4
print(f'{-5:+}')     # -5
import datetime
today = datetime.datetime.today()
print(f'{today:%B %d, %Y}')    # April 28, 2023

# send emails from CSV distribution
import csv
from email.message import EmailMessage
import smtplib

def get_credentials(filepath):
    with open("credentials.txt", "r") as f:
        email_address = f.readline()
        email_pass = f.readline()
    return (email_address, email_pass)

def login(email_address, email_pass, s):
    s.ehlo()
    # start TLS for security
    s.starttls()
    s.ehlo()
    # Authentication
    s.login(email_address, email_pass)
    print("login")

def send_mail():
    s = smtplib.SMTP("smtp.gmail.com", 587)
    email_address, email_pass = get_credentials("./credentials.txt")
    login(email_address, email_pass, s)
    # message to be sent
    subject = "Welcome to Python"
    body = """Python is an interpreted, high-level,
    general-purpose programming language.\n
    Created by Guido van Rossum and first released in 1991,
    Python's design philosophy emphasizes code readability\n
    with its notable use of significant whitespace"""
    message = EmailMessage()
    message.set_content(body)
    message['Subject'] = subject
    with open("emails.csv", newline="") as csvfile:
        spamreader = csv.reader(csvfile, delimiter=" ", quotechar="|")
        for email in spamreader:
            s.send_message(email_address, email[0], message)
            print("Send To " + email[0])
    # terminating the session
    s.quit()
    print("sent")

if __name__ == "__main__":
    send_mail()

# merged df with indicator
merged_df = names.merge(scores, on="id", how="outer", indicator=True)
# change suffix from x y
merged_df = products.merge(sales, on=["pg", "id"], suffixes=["_products", "_sales"])
# useful for time with seconds
merged_df = pd.merge_asof(df1, df2, on="time", direction="nearest")
merged_df = pd.merge_asof(df1, df2, on="time", allow_exact_matches=False)

# matriz loop
# Program to add two matrices using nested loop
 
X = [[1,2,3],
    [4 ,5,6],
    [7 ,8,9]]
 
Y = [[9,8,7],
    [6,5,4],
    [3,2,1]]
 
 
result = [[0,0,0],
        [0,0,0],
        [0,0,0]]
 
# iterate through rows
for i in range(len(X)):  
# iterate through columns
    for j in range(len(X[0])):
        result[i][j] = X[i][j] + Y[i][j]
 
for r in result:
    print(r)

#### Connect to smb

To change the directory and load a Hugging Face model using the smbclient library in Python, you need to follow these steps:
Connect to the Windows share drive:

from smb.SMBConnection import SMBConnection

# Create an SMB connection object
conn = SMBConnection('<username>', '<password>', '<client_name>', '<server_name>')

# Connect to the server
connected = conn.connect('<server_ip>', <server_port>)

Replace <username>, <password>, <client_name>, <server_name>, <server_ip>, and <server_port> with the appropriate values for your setup.
Change the current directory to the desired folder:

# Change to the desired directory path
conn.chdir('<directory_path>')

Replace <directory_path> with the path to the folder containing the Hugging Face model file.

Read and load the Hugging Face model:


from transformers import AutoModel

# Load the Hugging Face model file
model_path = '<model_file_name>'
model_file, _ = conn.retrieveFile('<model_path_on_share>', model_path)

# Load the model using the local file path
model = AutoModel.from_pretrained(model_path)

Replace <model_file_name> with the name of the Hugging Face model file (e.g., 'model.bin'). Also, make sure to specify <model_path_on_share> as the path to the model file on the share drive.
With these steps, you should be able to connect to the share drive, change the directory to the desired folder, and load the Hugging Face model using the smbclient library in Python.










# connect to ms access
https://datatofish.com/how-to-connect-python-to-ms-access-database-using-pyodbc/
https://stackoverflow.com/questions/12704305/return-column-names-from-pyodbc-execute-statement
https://stackoverflow.com/questions/16519385/output-pyodbc-cursor-results-as-python-dictionary
# concatenate rows with group by
https://stackoverflow.com/questions/27298178/concatenate-strings-from-several-rows-using-pandas-groupby
# update values from data in another dataframe
https://stackoverflow.com/questions/49928463/python-pandas-update-a-dataframe-value-from-another-dataframe
# factor lists
https://www.tutorialspoint.com/python-make-pair-from-two-list-such-that-elements-are-not-same-in-pairs
# add columns
https://towardsdatascience.com/10-ways-to-add-a-column-to-pandas-dataframes-ccadf7306d89

# load nltk stopwords

from nltk.corpus import stopwords

# Define the path to your custom stopwords file
stopwords_file_path = 'path/to/your/stopwords.txt'

# Load stopwords from the custom file
with open(stopwords_file_path, 'r') as f:
    custom_stopwords = set(f.read().splitlines())

# Add the custom stopwords to the NLTK stopwords set
nltk_stopwords = set(stopwords.words('english'))
nltk_stopwords.update(custom_stopwords)

# Now, nltk_stopwords contains the combined set of NLTK and custom stopwords

# load spacy model, Replace 'path/to/your/en_core_web_sm' with the actual path 
# to the directory where you've extracted the model. 
# This code will load the "en_core_web_sm" model from the specified local directory.

import spacy

# Replace 'path/to/your/model' with the actual path to the model directory
model_path = 'path/to/your/model'

nlp = spacy.load(model_path)

import datetime
import random

def schedule_workers(workers, time_segments):
  """
  Schedules workers into time segments.

  Args:
    workers: A list of workers.
    time_segments: A list of time segments.

  Returns:
    A list of tuples, where each tuple is a worker and the time segment they are assigned to.
  """

  # Create a list of available workers.
  available_workers = list(workers)

  # Create a list of assigned workers.
  assigned_workers = []

  # Loop through the time segments.
  for time_segment in time_segments:
    # If there are more available workers than time segments, select a random worker for each time segment.
    if len(available_workers) > len(time_segments):
      for time_segment in time_segments:
        worker = random.choice(available_workers)
        assigned_workers.append((worker, time_segment))
        available_workers.remove(worker)
    # Otherwise, assign the first available worker to each time segment.
    else:
      for time_segment in time_segments:
        worker = available_workers.pop()
        assigned_workers.append((worker, time_segment))

  return assigned_workers

if __name__ == "__main__":
  # Create a list of workers.
  workers = ["Worker 1", "Worker 2", "Worker 3", "Worker 4"]

  # Create a list of time segments.
  time_segments = [
    "9:00 AM - 10:00 AM",
    "10:00 AM - 11:00 AM",
    "11:00 AM - 12:00 PM",
    "12:00 PM - 1:00 PM",
    "1:00 PM - 2:00 PM",
    "2:00 PM - 3:00 PM",
    "3:00 PM - 4:00 PM",
    "4:00 PM - 5:00 PM"
  ]

  # Schedule the workers.
  assigned_workers = schedule_workers(workers, time_segments)

  # Print the scheduled workers.
  for worker, time_segment in assigned_workers:
    print(f"{worker} is scheduled for {time_segment}")